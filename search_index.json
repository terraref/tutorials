[["index.html", "TERRA REF Tutorials Chapter 1 Overview 1.1 Pre-requisites 1.2 Ways of Accessing Data 1.3 Other Resources", " TERRA REF Tutorials David LeBauer and others 2021-04-13 Chapter 1 Overview This book is intended to quickly introduce users to TERRA REF data through a series of tutorials. TERRA REF has many types of data, and most can be accessed in multiple ways. Although this makes it more complicated to learn (and teach!), the objective is to provide users with the flexibility to access data in the most useful way. The first section walks the user through the steps of downloading and combining three different types of data: plot level phenotypes, meteorological data, and images. Subsequent sections provide more detailed examples that show how to access a larger variety of data and meta-data. For those who are eager to see what is on deck, there are additional tutorials under development in the tutorials repository on GitHub. These can be found in the traits, sensors, and genomics subfolders. Users are encouraged to revise and contribute new content. 1.1 Pre-requisites While we assume that readers will have some familiarity with the nature of the problem - remote sensing of crop plants - for the most part, these tutorials assume that the user will bring their own scientific questions and a sense of curiosity and are eager to learn. These tutorials are aimed at users who are familiar with or willing to learn programming languages including R (particularly for accessing plot level trait data) and Python (primarily for accessing environmental data and sensor data). In addition, there are examples of using SQL for more sophisticated database queries, as well as the Bash terminal. Some of the lessons only require a web browser; others will assume familiarity with programming at the command line in (typically only one of) Python, R, and / or SQL. You should be willing to find help (see finding help, below). 1.1.1 Technical Requirements At a minimum, you should have: An internet connection Web browser Access to the data that you are using The tutorials will state which databases you will need access to Software: Software requirements vary with the tutorials, and may be complex 1.1.2 Installation instructions Option 1 The recommended and easier option is to use our VICE app on CyVerse’s Discovery Environment platform. All necessary programs and libraries are installed and accessed using RStudio. You’ll need a CyVerse account; follow instructions below to set up. Launching TERRA REF app on VICE: Click this button Click “Log in with your CyVerse ID” and log in using your CyVerse username and password Hit “Launch Analysis” button in pop up window, and select arrow button in Analyses window Wait patiently Sign into RStudio using username: rstudio and password: rstudio1 Reopen running TERRA REF instance: Open the CyVerse Discovery Environment Log in Open Analyses window and hit arrow button to open up RStudio Option 2 Everything necessary can be installed locally instead of using the VICE app. You will need to install R and a handful of R packages. All of the packages except traits are available on CRAN and can be installed using the install.packages function: install.packages(&#39;tidyverse&#39;, &#39;jsonlite&#39;, &#39;knitr&#39;, &#39;lubridate&#39;, &#39;raster&#39;, &#39;sf&#39;) Although traits is also on CRAN, you need to download the TERRA REF version from GitHub to handle larger datasets: install.packages(&#39;devtools&#39;) devtools::install_github(&#39;terraref/traits&#39;) You will additionally need to install Python and the terrautils library, as below: pip install terrautils 1.1.3 User accounts and permission to access TERRA REF data The first few chapters in the ‘vignettes’ section use publicly available sample data sets. Subsequent sections are also written to use publicly available data sets, but some of the examples require data that requires users to sign up. To sign up, you will need to 1) fill out the TERRA REF Beta user questionnaire (terraref.org/beta) and 2) request access to specific databases. 1.2 Ways of Accessing Data Web Interfaces Clowder (sensor and genomic data) Globus (sensor and genomic data) BETYdb (trait data and experimental metadata) CoGe (genomic data) Files Programming APIs BETYdb API API Clients rOpenSci traits package 1.3 Other Resources The TERRA REF website: terraref.org The TERRA REF Technical Documentation: docs.terraref.org 1.3.1 Finding help Slack at terra-ref.slack.com (signup). Browse issues and repositories in GitHub: search the organization at github.com/terraref questions about the tutorials in the tutorials repository about the data in the reference-data repository "],["vignettes-introduction.html", "Chapter 2 Vignettes Introduction", " Chapter 2 Vignettes Introduction These vignettes provide a quick start tutorial - they quickly show you how to access different types of data in the TERRA Reference datasets and perform ‘hello world’ tasks with them. The vignettes currently include examples of: trait data weather data image data combining trait, weather, and image data in a single analysis In order to make these vignettes as easy to use as possible, we are using a subset of the full dataset that is available without an account or password. Specifically, we have made data from thirty two plots in each of the two seasons available. Although we want to make these data broadly available, the remainder of the dataset requires an account to use so we can keep track of who and how many people are using it, and can contact users with their provided email addresses. Information for getting a beta user account is available in the section on User accounts. See Installation instructions in the Overview. "],["accessing-trait-data-in-r.html", "Chapter 3 Accessing trait data in R 3.1 Learning Objectives 3.2 Introduction 3.3 Query for available traits 3.4 Querying a specific trait", " Chapter 3 Accessing trait data in R 3.1 Learning Objectives In this chapter you will learn: How to create a summary of available data to query from a TERRA REF season How to query a specific trait How to visualize query results 3.2 Introduction In this chapter, we go over how to query TERRA REF trait data using the traits package. The traits package is a way to query for various sources of species trait data, including BETYdb, NCBI, Coral Traits Disease and others. In this chapter we use BETYdb as our trait source, as it contains the TERRA REF data that we are interested in. Our example will show how to query for season 6 data and visualize canopy height. In addition to the traits package we will also be using some of the tidyverse packages, which allow us to manipulate the data in an efficient, understandable way. If you are unfamiliar with tidyverse syntax, we recommend checking out some of the resources here. 3.3 Query for available traits 3.3.1 Getting Started First, we will need to install and load the traits package from CRAN, and load it into our environment, along with the other packages we will use in this tutorial. # install.packages(&#39;traits&#39;) # run once library(traits) library(ggplot2) library(lubridate) library(dplyr) library(knitr) 3.3.2 Setting options The function that is used to query BETYdb is called betydb_query. To reduce the number of arguments needed to pass into this function, we can set some global options using options. In this case, we will set the URL used in the query, and the API version. options(betydb_url = &quot;https://terraref.org/bety/&quot;, betydb_api_version = &#39;v1&#39;) 3.3.3 Querying available traits The TERRA REF database contains trait data for many other seasons of observation, and available data may vary by season. Here, we get a visual summary of available traits and methods of measurement for a season. First we construct a general query for the Season 4 data. This returns all season 4 data. The function betydb_query takes as arguments key = &quot;value&quot; pairs which represent columns in the database to query. In this example, we set sitename column for season 4 data, and set the limit to “none” to return all records. By default, the function will search all tables in the database. To specify a particular table you can use the table argument. # get all of season 4 data season_4 &lt;- betydb_query(sitename = &quot;~Season 4&quot;, limit = &quot;none&quot;) The return value for the betydb_query function is just a data.frame so we can work with it like any other data.frame in R. Let’s plot a time series of all traits returned. First you might notice that the relevant date columns in the season_4 data.frame are returned as characters instead of a date format. Before plotting, let’s get our raw_date column into a proper date format and time zone using functions from dplyr and lubridate. season_4 &lt;- season_4 %&gt;% mutate(trans_date = with_tz(ymd_hms(raw_date), &quot;America/Phoenix&quot;)) 3.3.4 Plot season 4 summary Now we can create a plot of all of the trait data collected during season 4, including information about the methods used. ggplot(data = season_4) + geom_point(aes(x = trans_date, y = mean, color = method_name), shape = &#39;.&#39;) + geom_line(aes(x = trans_date, y = mean, group = cultivar, color = method_name)) + facet_wrap(~trait, ncol = 4, scales = &quot;free_y&quot;) + xlab(&quot;Date&quot;) + ylab(&quot;Mean trait value&quot;) + ggtitle(&quot;Season 4 data summary&quot;) + guides(color = guide_legend(title=&quot;Method&quot;, ncol = 1, title.position = &quot;top&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) We can view more information about these trait measurements by examining unique values in the trait and trait description columns. traits &lt;- season_4 %&gt;% distinct(trait, trait_description) kable(traits) 3.4 Querying a specific trait 3.4.1 Querying season 6 canopy height data You may find after constructing a general query as above that you want to only query a specific trait. Here, we query for the canopy height trait by adding the key-value pair trait = &quot;canopy_height&quot; to our query function. Note that the limit is also set to return only 250 records, shown here for demonstration purposes. canopy_height &lt;- betydb_query(trait = &quot;canopy_height&quot;, sitename = &quot;~Season 6&quot;, limit = 250) 3.4.2 Plotting query results As before, we need to reformat the raw date column. canopy_height &lt;- canopy_height %&gt;% mutate(trans_date = with_tz(ymd_hms(raw_date), &quot;America/Phoenix&quot;)) And we can generate a time series plot of just the canopy height data. #plot a time series of canopy height ggplot(data = canopy_height, aes(x = trans_date, y = mean)) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + geom_smooth(size = 0.1) + xlab(&quot;Date&quot;) + ylab(&quot;Plant height (cm)&quot;) + ggtitle(&quot;Sorghum canopy height, Season 6 TERRA REF&quot;) + theme_bw() "],["accessing-weather-data-in-r.html", "Chapter 4 Accessing weather data in R 4.1 Objective: To be able to demonstrate how to get TERRA REF meteorological data 4.2 Read in data using R 4.3 Plot data using R", " Chapter 4 Accessing weather data in R 4.1 Objective: To be able to demonstrate how to get TERRA REF meteorological data This vignette shows how to read weather data for the month of January 2017 from the weather station at the University of Arizona’s Maricopa Agricultural Center into R. These data are stored online on the data management system Clowder, which is accessed using an API. More detailed information about the structure of the database and how API URLs are created is available in the weather tutorial. Data across time for one weather variable, temperature, is plotted in R. Then all eight of the weather variables have their times series plotted. 4.2 Read in data using R A set of weather data can be accessed with a URL using the R package jsonlite. We are calling that library along with several others that will be used to clean and plot the data. The data is read in by the fromJSON function as a dataframe that also has two nested dataframes, called properties and geometries. library(dplyr) library(ggplot2) library(jsonlite) library(lubridate) library(tidyr) weather_all &lt;- fromJSON(&#39;https://terraref.org/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2017-01-02&amp;until=2017-01-31&#39;, flatten = FALSE) The geometries dataframe is then pulled out from these data, which contains the datapoints from this stream. This is combined with a transformed version of the end of the time period in the correct time zone from the stream. weather_data &lt;- weather_all$properties %&gt;% mutate(time = with_tz(ymd_hms(weather_all$end_time), &quot;America/Phoenix&quot;)) The temperature data, which is five minute averages for the entire month of January 2017, is used to calculate the growing degree days for each day. Growing degree days is a measurement that is used to predict when certain plant developmental phases happen. This new dataframe will be used in the last vignette to synthesize the trait, weather, and image data. daily_values &lt;- weather_data %&gt;% mutate(date = as.Date(time), air_temp_converted = air_temperature - 273.15) %&gt;% group_by(date) %&gt;% summarise(min_temp = min(air_temp_converted), max_temp = max(air_temp_converted), gdd = ifelse(sum(min_temp, max_temp) / 2 &gt; 10, (max_temp + min_temp) / 2 - 10, 0)) 4.3 Plot data using R The five minute summary weather variables in the weather_data dataframe can be plotted across time, as shown below for temperature. ggplot(data = weather_data) + geom_point(aes(x = time, y = air_temperature), size = 0.1) + labs(x = &quot;Date&quot;, y = &quot;Temperature (K)&quot;) We can also plot the time series for all eight of the weather variables in a single figure. We first have to rearrange the data to making plotting possible using R package ggplot. weather_data_long &lt;- weather_data %&gt;% select(-source, -source_file) %&gt;% gather(weather_variable, observation, -time) ggplot(data = weather_data_long) + geom_point(aes(x = time, y = observation), size = 0.1) + facet_wrap(~weather_variable, scales = &quot;free_y&quot;) + labs(x = &quot;Date&quot;, y = &quot;Weather variable&quot;) You should now be able to find, get, and use weather data from the TERRA REF project via Clowder. "],["retrieve-source-rgb-image-files.html", "Chapter 5 Retrieve source RGB image files 5.1 Learning Objective 5.2 Introduction 5.3 Getting Started 5.4 Retrieving sensor names 5.5 Retrieving the images 5.6 Sample Images", " Chapter 5 Retrieve source RGB image files 5.1 Learning Objective In this chapter you will learn: Find and retrieve a list of available sensor names Retrieve files by downloading them to your system 5.2 Introduction In this chapter we will show how to use the Python terrutils library to retrieve the list of available sensor names. The terrautils library provides a number of functions that can be used to perform different actions with data that is stored in TerraRef. The names of the sensors we retrieve using TerraUtils can provide information on what types of Level 1 data is available. Our examples also show how to retrieve files of interest from TerraUtils by using the available API (Application Programming Interface). 5.3 Getting Started First, we will need to install the terrautils library into the Python environment. We can do this by using the pip utility to install the library from pypi. Simple run pip install terrautils in a terminal to install terrautils. All the terrautils functions are now available in Python, although we will only use a very limited number of them. 5.4 Retrieving sensor names In this section we retrieve the names of different sensor types that are available. This will allow you to understand what files may be available other than just those containing RBG image data. In order to run Python functions, including those from the terrautils library, within this Rmarkdown, we have to install and set up reticulate. if(!require(reticulate)){ install.packages(&quot;reticulate&quot;) reticulate::py_install(&quot;terrautils&quot;) } library(reticulate) use_virtualenv(&quot;r-reticulate&quot;) We will first be using the get_sensor_list function to retrieve all the data on available sensors. We will then use the unique_sensor_names function to extract only the sensor names from the data we just retrieved. from terrautils.products import get_sensor_list, unique_sensor_names url = &#39;https://terraref.ncsa.illinois.edu/clowder/&#39; key = &#39;&#39; sensors = get_sensor_list(None, url, key) names = unique_sensor_names(sensors) The variable names will now contain the list of all available sensors. 5.5 Retrieving the images Once we have a list of files and their IDs we can retrieve them one-by-one. We do this by creating a URL that identifies the file to retrieve, making the API call to retrieve the file contents, and writing the contents to disk. To create the correct URL we start with the one defined before and attach the keyword ‘/files/’ followed by the ID of each file. For example, assuming we have a file ID of ‘111’, the final URL for retrieving the file would be: https://terraref.ncsa.illinois.edu/clowder/api/files/111 By looping through each of our files, and using their ID and filename, we can retrieve the files from the server and store them locally. We are streaming the data returned from our server request (stream=True in the code below) due to the high probability of large file sizes. If the stream=True parameter was omitted the file’s entire contents would be in the r variable which could then be written to the local file. To illustrate how this might work we are going to pre-populated an array of file names and their associated Clowder IDs. files = [ {&quot;id&quot;: &quot;5c507cb74f0c4b0cbe6705f2&quot;, &quot;filename&quot;: &quot;rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_right.tif&quot;}, {&quot;id&quot;: &quot;5c507cb84f0cfd2aedf5a75a&quot;, &quot;filename&quot;: &quot;rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_left.tif&quot;}, {&quot;id&quot;: &quot;5c507eaf4f0c4b0cbe6716cd&quot;, &quot;filename&quot;: &quot;rgb_geotiff_L1_ua-mac_2018-05-05__11-35-13-442_left.tif&quot;}, {&quot;id&quot;: &quot;5c507eaf4f0cfd2aedf5b680&quot;, &quot;filename&quot;: &quot;rgb_geotiff_L1_ua-mac_2018-05-05__11-37-40-442_right.tif&quot;} ] The following code shows how to download the image files. First we format the base URL for our query allowing us to reuse it for each file. Next we loop through our array and create a customized URL while making the call to fetch the data using the requests interface. Finally we open the output file and use a loop to write the retrieved data. import requests from io import open # We are using the same `url` and `key` variables declared in the previous example above. filesurl = url + &#39;files/&#39; params={ &#39;key&#39;: key } for f in files: r = requests.get(filesurl + f[&quot;id&quot;], params=params, stream=True) with open(f[&quot;filename&quot;], &#39;wb&#39;) as o: for chunk in r.iter_content(chunk_size=1024): if chunk: o.write(chunk) The images are now stored on the local file system. 5.6 Sample Images Below are examples of images captured approximately one month apart 1 2 Date Images May 4, 2018 Jun 2, 2018 May 4, 2018 - rgb_geotiff_L1_ua-mac_2018-05-04__13-07-04-077_right.tif,rgb_geotiff_L1_ua-mac_2018-05-04__13-07-04-077_left.tif↩ Jun 2, 2018 - rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_right.tif,rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_left.tif↩ "],["combining-trait-weather-and-image-datasets.html", "Chapter 6 Combining trait, weather, and image datasets 6.1 Get and join data 6.2 Plot and model relationship between GDD and canopy cover for each cultivar 6.3 Create histogram of growth rate for all cultivars 6.4 Get image data", " Chapter 6 Combining trait, weather, and image datasets The objective of this vignette is to walk through how to combine our several types of data, and demonstrate several realistic analyses that can be done on these merged data. For the first analysis, we want to figure out how the number of sufficiently warm days affects the amount of canopy cover at our site. We do this by combining the canopy cover data with the meteorological data on growing degree days, then modeling and plotting their relationship. We are specifically interested in figuring out when the increase in canopy cover starts to slow down in response to warm temperature days. The second analysis compares greenness from image data with canopy cover. 6.1 Get and join data Here we combine two dataframes. The first contains all the canopy cover values for 2018, which was created in the traits vignette. The second is the cumulative growing degree days for all of 2018, which were calculated from the daily minimum and maximum temperatures in the weather vignette. They are combined by their common column, the date. library(dplyr) library(ggplot2) library(jsonlite) library(lubridate) library(traits) library(sf) library(stringr) options(betydb_url = &quot;https://terraref.ncsa.illinois.edu/bety/&quot;, betydb_api_version = &#39;beta&#39;, betydb_key = &#39;9999999999999999999999999999999999999999&#39;) trait_canopy_cover &lt;- betydb_query(table = &quot;search&quot;, trait = &quot;canopy_cover&quot;, date = &quot;~2018&quot;, limit = &quot;none&quot;) trait_canopy_cover_day = trait_canopy_cover %&gt;% mutate(trans_date = with_tz(ymd_hms(raw_date), &quot;America/Phoenix&quot;), day = as.Date(raw_date)) weather &lt;- fromJSON(&#39;https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2018-01-01&amp;until=2018-12-31&#39;, flatten = FALSE) weather &lt;- weather$properties %&gt;% mutate(time = with_tz(ymd_hms(weather$end_time), &quot;America/Phoenix&quot;)) daily_values = weather %&gt;% mutate(day = as.Date(time), air_temp_converted = air_temperature - 273.15) %&gt;% group_by(day) %&gt;% summarise(min_temp = min(air_temp_converted), max_temp = max(air_temp_converted), gdd = ifelse(sum(min_temp, max_temp) / 2 &gt; 10, (max_temp + min_temp) / 2 - 10, 0)) daily_values &lt;- daily_values %&gt;% mutate(gdd_cum = cumsum(gdd)) trait_weather_df &lt;- full_join(trait_canopy_cover_day, daily_values, by = &quot;day&quot;) %&gt;% select(day, cultivar, mean, gdd_cum) %&gt;% na.omit() 6.2 Plot and model relationship between GDD and canopy cover for each cultivar We are interested in how growing degree days affects canopy cover. To investigate this, we are going to model and plot their relationship. We are using a logistic growth model here because it is appropriate for the shape of the GDD-cover relationship. The logistic growth model is specified as \\[y = \\frac{c}{1+e^{a + b * \\textrm{x}}}\\] where \\(y\\) is the response variable canopy cover, \\(x\\) is the predictor growing degree days, \\(c\\) is the asymptote or maximum canopy cover, \\(a\\) is the initial value for canopy cover, and \\(b\\) is the steepness of the curve. (reference) We want to know the relationship for each cultivar, so we’ll start of by determining the parameters of the model for one of the cultivars in our dataset. We provide estimated values for the asymptote \\(c\\) and initial canopy cover value \\(a\\), and provide canopy cover \\(y\\) with corresponding growing degree days \\(x\\) for one measurement of the chosen cultivar. The below provides better estimates for the \\(c\\), \\(a\\), and \\(b\\) parameters, which are used to plot the model as an orange line on top of the black points which are actual values. single_cultivar &lt;- trait_weather_df %&gt;% filter(cultivar == &quot;PI656026&quot;) c &lt;- 90 a &lt;- 0.1 y &lt;- single_cultivar$mean[3] g &lt;- single_cultivar$gdd_cum[3] b &lt;- ((log((c/y) - 1)) - a)/g model_single_cultivar &lt;- nls(mean ~ c / (1 + exp(a + b * gdd_cum)), start = list(c = c, a = a, b = b), data = single_cultivar) summary(model_single_cultivar) coef(model_single_cultivar) single_c &lt;- coef(model_single_cultivar)[1] single_a &lt;- coef(model_single_cultivar)[2] single_b &lt;- coef(model_single_cultivar)[3] single_cultivar &lt;- single_cultivar %&gt;% mutate(mean_predict = single_c / (1 + exp(single_a + single_b * gdd_cum))) ggplot(single_cultivar) + geom_point(aes(x = gdd_cum, y = mean)) + geom_line(aes(x = gdd_cum, y = mean_predict), color = &quot;orange&quot;) + labs(x = &quot;Cumulative growing degree days&quot;, y = &quot;Canopy Height&quot;) We then calculate the inflection point for this cultivar’s model. The maximum growth rate is the change in canopy cover per day at the rate of maximum growth. The growing degree day at which maximum growth is obtained is called the inflection point. This occurs near the midpoint of the y-axis, or \\(\\frac{c - a}{2}\\). inf_y &lt;- (as.numeric(single_c) - as.numeric(single_a)) / 2 inf_x &lt;- ((log((as.numeric(single_c) / inf_y) - 1)) - as.numeric(single_a)) / as.numeric(single_b) ggplot(single_cultivar) + geom_point(aes(x = gdd_cum, y = mean)) + geom_line(aes(x = gdd_cum, y = mean_predict), color = &quot;orange&quot;) + geom_hline(yintercept = inf_y, linetype = &quot;dashed&quot;) + geom_vline(xintercept = inf_x) + labs(x = &quot;Cumulative growing degree days&quot;, y = &quot;Canopy Height&quot;) We then use the parameters from a single cultivar to run a model for each of the rest of the cultivars. These results are used to plot the model predictions, which are shown as an orange line. We also calculated the inflection point from each cultivar’s model, which will be used in the following section. all_cultivars &lt;- c(day = as.double(), cultivar = as.character(), mean = as.numeric(), gdd_cum = as.numeric(), mean_predict = as.numeric(), inf_y = as.numeric(), inf_x = as.numeric()) for(each_cultivar in unique(trait_weather_df$cultivar)){ each_cultivar_df &lt;- filter(trait_weather_df, cultivar == each_cultivar) each_cultivar_model &lt;- nls(mean ~ c / (1 + exp(a + b * gdd_cum)), start = list(c = c, a = a, b = b), data = each_cultivar_df) model_c &lt;- coef(each_cultivar_model)[1] model_a &lt;- coef(each_cultivar_model)[2] model_b &lt;- coef(each_cultivar_model)[3] each_cultivar_df &lt;- each_cultivar_df %&gt;% mutate(mean_predict = model_c / (1 + exp(model_a + model_b * gdd_cum)), inf_y = (as.numeric(model_c) - as.numeric(model_a)) / 2, inf_x = ((log((as.numeric(model_c) / inf_y) - 1)) - as.numeric(single_a)) / as.numeric(single_b)) all_cultivars &lt;- rbind(each_cultivar_df, all_cultivars) } ggplot(all_cultivars) + geom_point(aes(x = gdd_cum, y = mean)) + geom_line(aes(x = gdd_cum, y = mean_predict), color = &quot;orange&quot;) + facet_wrap(~cultivar, scales = &quot;free_y&quot;) + geom_hline(yintercept = inf_y, linetype = &quot;dashed&quot;) + geom_vline(xintercept = inf_x) + labs(x = &quot;Cumulative growing degree days&quot;, y = &quot;Canopy Height&quot;) 6.3 Create histogram of growth rate for all cultivars The last thing that we are going to do is assess the difference in this relationship among the cultivars. We are going to use the inflection point from the logistic growth model, which indicates when canopy cover stops increasing as quickly with increasingly more warm days. The resulting inflection points for each cultivar are plotted as a histogram. ggplot(data.frame(inf_points = unique(all_cultivars$inf_x))) + geom_histogram(aes(x = inf_points), bins = 300) + xlim(min(all_cultivars$gdd_cum), max(all_cultivars$gdd_cum)) + labs(x = &quot;Inflection points&quot;, y = &quot;Number&quot;) 6.4 Get image data In this example we will extract our plot data from a series of images taken in May of Season 6, measure its “greeness” annd plot that against the plant heights from above in this vignette. The chosen statistic here is the normalised green-red difference index, \\(\\textrm{NGRDI}=\\frac{R-G}/{R+G}\\) (Rasmussen et al., 2016), which uses the red and green bands from the image raster. Below we retrieve all the available plots for a particular date, then find and convert the plot boundary JSON into tuples. We will use these tuples to extract the data for our plot. # Making the query for our site sites &lt;- betydb_query(table = &quot;sites&quot;, sitename = &quot;MAC Field Scanner Season 6 Range 19 Column 1&quot;) # Assigning the geometry of the site (GeoJSON format) site.geom &lt;- sites$geometry # Convert the polygon to something we can clip with. CRS value represents WGS84 Lat/Long site.shape &lt;- st_as_sfc(site.geom,crs = 4326) site.poly &lt;- st_cast(site.shape, &quot;POINT&quot;) site.clip &lt;- as(site.poly,&quot;Spatial&quot;) These are the names of the full field RGB data for the month of May. We will be extracting our plot data from these files. A compressed file containing these images can be found on Clowder. The code below downloads the image files into a .zip file, which takes a few minutes, and then unzips that file so the image files are accessible. if(!file.exists(&quot;rgb_images.zip&quot;)){ download.file(&quot;https://terraref.ncsa.illinois.edu/clowder/files/5c8175874f0c78f6486d6870/blob&quot;, destfile = &quot;rgb_images.zip&quot;) unzip(&quot;rgb_images.zip&quot;, exdir = &quot;.&quot;) } We will loop through these images, extract our plot data, and calculate the “greeness” of each extract. We are using the name of the file to extract the date for later. library(raster) # Get file paths for all image files image_files &lt;- list.files(&quot;.&quot;, pattern = &quot;*.tif&quot;) image_files_paths &lt;- file.path(&quot;.&quot;, image_files) # Extract the date from the file name getDate &lt;- function(file_name){ date &lt;- str_match_all(file_name, &#39;[0-9]{4}-[0-9]{2}-[0-9]{2}&#39;)[[1]][,1] return(date) } # Returns the greeness value of the plot in the specified file getGreeness &lt;- function(file_name, clip_coords){ band_image_red &lt;- raster(file_name, band = 1) red_crop &lt;- crop(band_image_red, clip_coords) band_image_green &lt;- raster(file_name, band = 2) green_crop &lt;- crop(band_image_green, clip_coords) add_rasters &lt;- green_crop + red_crop numerator &lt;- cellStats(add_rasters, stat = &quot;sum&quot;) subtract_rasters &lt;- green_crop - red_crop denominator &lt;- cellStats(subtract_rasters, stat = &quot;sum&quot;) greeness &lt;- numerator / denominator return(greeness) } # Extract all the dates from the images date &lt;- sapply(image_files_paths, getDate, USE.NAMES = FALSE) # Extract all the greeness for the plot greeness &lt;- sapply(image_files_paths, getGreeness, clip_coords=site.clip, USE.NAMES = FALSE) # Build the final day and greeness greenness_df &lt;- data.frame(date, greeness) %&gt;% as_tibble() %&gt;% mutate(day = as.Date(date)) We then pull in the canopy data for our charting purposes. trait_canopy_cover &lt;- betydb_query(table = &quot;search&quot;, trait = &quot;canopy_cover&quot;, date = &quot;~2018 May&quot;, limit = &quot;none&quot;) trait_canopy_cover_day &lt;- trait_canopy_cover %&gt;% mutate(trans_date = with_tz(ymd_hms(raw_date), &quot;America/Phoenix&quot;), day = as.Date(raw_date)) We now need to add the height data to the data set to plot. We then determine the average canopy cover across the site for the day that the sensor data were collected. The relationship between our greenness metric and average canopy cover are plotted. trait_canopy_cover_daily &lt;- trait_canopy_cover_day %&gt;% filter(day %in% greenness_df$day) %&gt;% group_by(day) %&gt;% summarise(mean_canopy_cover = mean(mean), sd_canopy_cover = sd(mean)) sensor_trait_df &lt;- left_join(trait_canopy_cover_daily, greenness_df, by = &quot;day&quot;) ggplot(sensor_trait_df, aes(x = mean_canopy_cover, y = greeness)) + geom_point() "],["accessing-trait-data-in-r-1.html", "Chapter 7 Accessing Trait Data in R 7.1 Using the R traits package to query the database", " Chapter 7 Accessing Trait Data in R The rOpenSci traits package makes it easier to query the TERRA REF trait database because 1) you can pass the query parameters in an R function, and the package takes care of putting the parameters into a valid URL and 2) because the package returns data in a tabular format that is ready to analyze. 7.1 Using the R traits package to query the database 7.1.1 Setup Install the traits package The traits package can be installed through github using the following command: if(packageVersion(&quot;traits&quot;) == &#39;0.2.0&#39;){ devtools::install_github(&#39;terraref/traits&#39;, force = TRUE) } Load other packages that we will need to get started. library(traits) library(ggplot2) library(ggthemes) library(dplyr) library(lubridate) theme_set(theme_bw()) Create a file that contains your API key. If you have signed up for access to the TERRA REF database, your API key will have been sent to you in an email. You will need this personal key and permissions to access the trait data. If you receive empty (NULL) datasets, it is likely that you do not have permissions. # This should be done once with the key sent to you in your email # Example: #writeLines(&#39;abcdefg_rest_of_key_sent_in_email&#39;, # con = &#39;.betykey&#39;) 7.1.1.1 R - using the traits package The R traits package is an API ‘client’. It does two important things: 1. It makes it easier to specify the query parameters without having to construct a URL 2. It returns the results as a data frame, which is easier to use within R Lets start with the query of information about Sorghum from the species table sorghum_info &lt;- betydb_query(table = &#39;species&#39;, genus = &quot;Sorghum&quot;, api_version = &#39;v1&#39;, limit = &#39;none&#39;, betyurl = &quot;https://terraref.org/bety/&quot;) 7.1.1.2 R - setting options for the traits package Notice all of the arguments that the betydb_query function requires? We can change this by setting the default connection options thus: options(betydb_url = &quot;https://terraref.org/bety/&quot;, betydb_api_version = &#39;v1&#39;) Now the same query can be reduced to: sorghum_info &lt;- betydb_query(table = &#39;species&#39;, genus = &quot;Sorghum&quot;, limit = &#39;none&#39;) 7.1.2 Example: Time series of height Now let’s query some trait data. canopy_height &lt;- betydb_query(table = &#39;search&#39;, trait = &quot;canopy_height&quot;, sitename = &quot;~Season 6&quot;, limit = &#39;none&#39;) First let’s fix the raw_date column so that it is represented as an actual date object using lubridate::ymd_hms. It is also converted to the correct time zone with with_tz, another lubridate function. canopy_height &lt;- canopy_height %&gt;% mutate(trans_date = with_tz(ymd_hms(raw_date), &quot;America/Phoenix&quot;)) ggplot(data = canopy_height, aes(x = trans_date, y = mean)) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + xlab(&quot;Date&quot;) + ylab(&quot;Plant Height&quot;) + guides(color = guide_legend(title = &#39;Genotype&#39;)) + theme_bw() "],["accessing-meteorological-data.html", "Chapter 8 Accessing meteorological data 8.1 The Maricopa Weather Station 8.2 Weather Plots", " Chapter 8 Accessing meteorological data Objectives: This tutorial will walk through the steps required to access meteorological data from the Maricopa Agricultural Center. Pre-requisites: Need to have R packages tidyverse, jsonlite, and convertr installed. Need to have an internet connection. 8.1 The Maricopa Weather Station 8.1.1 Meteorological data formats 8.1.1.1 Dimensions: CF standard-name units time days since 1970-01-01 00:00:00 UTC longitude degrees_east latitude degrees_north 8.1.1.2 Variable names and units CF standard-name units bety isimip cruncep narr ameriflux air_temperature K airT tasAdjust tair air TA (C) air_pressure Pa air_pressure PRESS (KPa) mole_fraction_of_carbon_dioxide_in_air mol/mol CO2 relative_humidity % relative_humidity rhurs NA rhum RH surface_downwelling_photosynthetic_photon_flux_in_air mol m-2 s-1 PAR PAR (NOT DONE) precipitation_flux kg m-2 s-1 cccc prAdjust rain acpc PREC (mm/s) degrees wind_direction WD wind_speed m/s Wspd WS variable names are from MsTMIP. standard_name is CF-convention standard names units can be converted by udunits, so these can vary (e.g. the time denominator may change with time frequency of inputs) soil moisture for the full column, rather than a layer, is soil_moisture_content For example, in the MsTMIP-CRUNCEP data, the variable rain should be precipitation_rate. We want to standardize the units as well as part of the met2CF.&lt;product&gt; step. I believe we want to use the CF “canonical” units but retain the MsTMIP units any time CF is ambiguous about the units. The key is to process each type of met data (site, reanalysis, forecast, climate scenario, etc) to the exact same standard. This way every operation after that (extract, gap fill, downscale, convert to a model, etc) will always have the exact same inputs. This will make everything else much simpler to code and allow us to avoid a lot of unnecessary data checking, tests, etc being repeated in every downstream function. 8.1.2 Using the API to get data In order to access the data, we need to contruct a URL that links to where the data is located on Clowder. The data is then pulled down using the API, which “receives requests and sends responses” , for Clowder. 8.1.3 The structure of the Geostreams database The meteorological data that is collected for the TERRA REF project is contained in multiple related tables, also know as a relational database. The first table contains data about the sensor that is collecting data. This is then linked to a stream table, which contains information about a datastream from the sensor. Sensors can have multiple datastreams. The actual weather data is in the third table, the datapoint table. A visual representation of this structure is shown below. In this vignette, we will be using data from a weather station at the Maricopa Agricultural Center, with datapoints for the month of January 2017 from a certain sensor. These data are five minute summaries aggregated from observations taken every second. 8.1.4 Creating the URLs for all data table types All URLs have the same beginning (https://terraref.org/clowder/api/geostreams), then additional information is added for each type of data table as shown below. Station: /sensors/sensor_name=[name] Sensor: /sensors/[sensor number]/streams Datapoints: /datapoints?stream_id=[datapoints number]&amp;since=[start date]&amp;until=[end date] A certain time period can be specified for the datapoints. For example, below are the URLs for the particular data being used in this vignette. These can be pasted into a browser to see how the data is stored as text using JSON. Station: https://terraref.org/clowder/api/geostreams/sensors?sensor_name=UA-MAC+AZMET+Weather+Station Sensor: https://terraref.org/clowder/api/geostreams/sensors/438/streams Datapoints: https://terraref.org/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2017-01-02&amp;until=2017-01-31 Possible sensor numbers for a station are found on the page for that station under “id:”, and then datapoints numbers are found on the sensor page under “stream_id:”. The table belows lists the names of some stations that have available meteorological data and associated stream ids. stream id name 3212 Irrigation Observations 46431 Weather Observations (5 min bins) 3208 EnvironmentLogger sensor_weather_station 3207 EnvironmentLogger sensor_par 748 EnvironmentLogger sensor_spectrum 3210 EnvironmentLogger sensor_co2 4806 UIUC Energy Farm SE 4807 UIUC Energy Farm CEN 4805 UIUC Energy Farm NE Here is the json representation of a single five-minute observation: [ { &quot;geometry&quot;:{ &quot;type&quot;:&quot;Point&quot;, &quot;coordinates&quot;:[ 33.0745666667, -111.9750833333, 0 ] }, &quot;start_time&quot;:&quot;2016-08-30T00:06:24-07:00&quot;, &quot;type&quot;:&quot;Feature&quot;, &quot;end_time&quot;:&quot;2016-08-30T00:10:00-07:00&quot;, &quot;properties&quot;:{ &quot;precipitation_rate&quot;:0.0, &quot;wind_speed&quot;:1.6207870370370374, &quot;surface_downwelling_shortwave_flux_in_air&quot;:0.0, &quot;northward_wind&quot;:0.07488770951583902, &quot;relative_humidity&quot;:26.18560185185185, &quot;air_temperature&quot;:300.17606481481516, &quot;eastward_wind&quot;:1.571286062845733, &quot;surface_downwelling_photosynthetic_photon_flux_in_air&quot;:0.0 } }, 8.1.5 Querying weather sensor data stream The data represent 5 minute summaries aggregated from 1/s observations. 8.1.6 Download data using the command line Data can be downloaded from Clowder using the command line program Curl. If the following is typed into the command line, it will download the datapoints data that we’re interested in as a file which we have chosen to call spectra.json. curl -o spectra.json -X GET https://terraref.org/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2017-01-02&amp;until=2017-01-31 8.1.6.1 Using R The following code sets the defaults for showing R code. knitr::opts_chunk$set(cache = FALSE, message = FALSE) And this is how you can access the same data in R. This uses the jsonlite R package and desired URL to pull the data in. The data is in a dataframe with two nested dataframes, called properties and geometries. library(dplyr) library(ggplot2) library(jsonlite) library(lubridate) library(magrittr) library(RCurl) library(ncdf4) library(ncdf.tools) weather_all &lt;- fromJSON(&#39;https://terraref.org/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2018-04-01&amp;until=2018-08-01&#39;, flatten = FALSE) The geometries dataframe is then pulled out from these data, which contains the datapoints from this stream. This is combined with a transformed version of the end of the time period from the stream. weather_data &lt;- weather_all$properties %&gt;% mutate(time = with_tz(ymd_hms(weather_all$end_time), &quot;America/Phoenix&quot;)) 8.2 Weather Plots Create time series plot for one of the eight variables, wind speed, in the newly created dataframe. theme_set(ggthemes::theme_few()) ggplot(data = weather_data) + geom_point(aes(x = time, y = wind_speed), size = 0.7) + labs(x = &quot;Day&quot;, y = &quot;Wind speed (m/s)&quot;) 8.2.1 High resolution data (1/s) + spectroradiometer This higher resolution weather data can be used for VNIR calibration, for example. But at 1/s it is very large! 8.2.1.1 Download data Here we will download the files using the Clowder API, but note that if you have access to the filesystem on Globus, you can directly access the data in the sites/ua-mac/Level_1/EnvironmentLogger folder. knitr::opts_chunk$set(eval = FALSE) api_url &lt;- &quot;https://terraref.org/clowder/api&quot; output_dir &lt;- file.path(tempdir(), &quot;downloads&quot;) dir.create(output_dir, showWarnings = FALSE, recursive = TRUE) # Get Spaces from Clowder - without authentication, result will be Sample Data spaces &lt;- fromJSON(paste0(api_url, &#39;/spaces&#39;)) print(spaces %&gt;% select(id, name)) # Get list of (at most 20) Datasets within that Space from Clowder datasets &lt;- fromJSON(paste0(api_url, &#39;/spaces/&#39;, spaces$id, &#39;/datasets&#39;)) print(datasets %&gt;% select(id, name)) # Get list of Files within any EnvironmentLogger datasets and filter .nc files files &lt;- fromJSON(paste0(api_url, &#39;/datasets/&#39;, datasets$id[grepl(&quot;EnvironmentLogger&quot;, datasets$name)], &#39;/files&#39;)) ncfiles &lt;- files[grepl(&#39;environmentlogger.nc&#39;, files$filename), ] print(ncfiles %&gt;% select(id, filename)) 8.2.1.2 Download netCDF 1/s data from Clowder 8.2.1.3 Using the netCDF 1/s data One use case getting the solar spectrum associated with a particular hyperspectral image. time &lt;- vector() vals &lt;- vector() for (i in 1:length(outputs)) { print(paste0(&quot;Scanning &quot;, outputs[i])) ncfile &lt;- nc_open(outputs[i]) curr_time &lt;- list() metdata &lt;- list() for(var in c(names(ncfile$dim), names(ncfile$var))){ metdata[[var]] &lt;- ncvar_get(ncfile, var) } lapply(metdata, dim) days &lt;- ncvar_get(ncfile, varid = &quot;time&quot;) curr_time &lt;- as.numeric(ymd(&quot;1970-01-01&quot;) + seconds(days * 24 * 60 * 60)) time &lt;- c(time, curr_time) PAR &lt;- c(vals, metdata$`par_sensor/Sensor_Photosynthetically_Active_Radiation`) } #ggplot() + # geom_line(aes(time, PAR)) + theme_bw() print(ncfile) "],["generating-file-lists-by-plot.html", "Chapter 9 Generating file lists by plot 9.1 Pre-requisites: 9.2 Getting started 9.3 Getting the sensor list 9.4 Getting a list of files 9.5 Querying the API", " Chapter 9 Generating file lists by plot 9.1 Pre-requisites: if you have not already done so, you will need to 1) sign up for the beta user program and 2) sign up and be approved for access to the the sensor data portal in order to get the API key that will be used in this tutorial. The terrautils python package has a new products module that aids in connecting plot boundaries stored within betydb with the file-based data products available from Globus. if are using Rstudio and want to run the Python code chunks, the R package “reticulate” is required use pip3 install terrautils to install the terrautils Python library 9.2 Getting started After installing terrautils, you should be able to import the products module. from terrautils.products import get_sensor_list, unique_sensor_names from terrautils.products import get_file_listing, extract_file_paths The get_sensor_list and get_file_listing functions both require the connection, url, and key parameters. The connection can be ‘None’. The url (called host in the code) should be something like https://terraref.org/clowder/. The key is a unique access key for the Clowder API. 9.3 Getting the sensor list The first thing to get is the sensor name. This can be retrieved using the get_sensor_list function. This function returns the full record which may be useful in some cases but primarily includes sensor names that include a plot id number. The utility function unique_sensor_names accepts the sensor list and provides a list of names suitable for use in the get_file_listing function. To use this tutorial you will need to sign up for Clowder, have your account approved, and then get an API key from the Clowder web interface. url = &#39;https://terraref.org/clowder/&#39; key = &#39;ENTER YOUR KEY HERE&#39; sensors = get_sensor_list(None, url, key) names = unique_sensor_names(sensors) print(names) Names will now contain a list of sensor names available in the Clowder geostreams API. The list of returned sensor names could be something like the following: flirIrCamera Datasets IR Surface Temperature RGB GeoTIFFs Datasets stereoTop Datasets scanner3DTop Datasets Thermal IR GeoTIFFs Datasets … 9.4 Getting a list of files The geostreams API can be used to get a list of datasets that overlap a specific plot boundary and, optionally, limited by a time range. Iterating over the datasets allows the paths to all the files to be extracted. sensor = &#39;Thermal IR GeoTIFFs Datasets&#39; sitename = &#39;MAC Field Scanner Season 1 Field Plot 101 W&#39; key = &#39;INSERT YOUR KEY HERE&#39; datasets = get_file_listing(None, url, key, sensor, sitename) files = extract_file_paths(datasets) Datasets can be further filtered using the since and until parameters of get_file_listing with a date string. dataset = get_file_listing(None, url, key, sensor, sitename, since=&#39;2016-06-01&#39;, until=&#39;2016-06-10&#39;) 9.5 Querying the API The source files behind the data are available for downloading through the API. By executing a series of requests against the API it’s possible to determine the files of interest and then download them. Each of the API URL’s have the same beginning (https://terraref.org/clowder/api), followed by the data needed for a specific request. As we step through the process you will be able to see how then end of the URL changes depending upon the request. Below is what the API looks like as a URL. Try pasting it into your browser. https://terraref.org/clowder/api/geostreams/sensors?sensor_name=MAC Field Scanner Season 1 Field Plot 101 W This will return data for the requested plot including its id. This id (or identifier) can then be used for additional queries against the API. In the examples below we will be using curl on the command line to make our API calls. Since the API is accessed through URLs, it’s possible to use the URLs in software programs or with a programming language to retrieve its data. 9.5.1 A Word of Caution We are no longer using the python terrautils package, which is a python library that provides helper functions that simplify interactions with the Clowder API. One of the ways it makes the interface easier is by using function names that make sense in the scope of the project. The API and the Clowder database have different names and this is confusing since the same names are used for different parts of the database. The names and meanings of variables in this section don’t necessarily match the ones in the section above and it may be easy to get them confused. The API queries the database directly and thereby reflects the database structure. This is the main reason for the naming differences between the API and the terraref client. For example, the Clowder API’s use of the term SENSOR_NAME is equivalent to site_name above. 9.5.2 Finding plot ID We can query the API to find the identifier associated with the name of a plot. For this example we use the variable name of SENSOR_DATA to indicate the name of the plot. SENSOR_NAME=&quot;MAC Field Scanner Season 1 Field Plot 101 W&quot; curl -o plot.json -X GET &quot;https://terraref.org/clowder/api/geostreams/sensors?sensor_name=${SENSOR_NAME}&quot; This creates a file named plot.json containing the JSON object returned by the API. The JSON object has an ‘id’ parameter. This ID parameter can be used to specify the correct data stream. 9.5.3 Finding stream ID within a plot Using the sensor ID returned in the JSON from the previous call and the id of a sensor returned previously to get the stream id. The names of streams are are formatted as “ Datasets ()”. SENSOR_ID=3355 STREAM_NAME=&quot;Thermal IR GeoTIFFs Datasets (${SENSOR_ID})&quot; curl -o stream.json -X GET &quot;https://terraref.org/clowder/api/geostreams/streams?stream_name=${STREAM_NAME}&quot; A file named stream.json will be created containing the returned JSON object. This JSON object has an ‘id’ parameter that contains the stream ID. You can use this ID parameter to get the datasets, and then datapoints, of interest. 9.5.4 Listing Clowder dataset IDs for that plot &amp; sensor stream We now have a stream ID that we can use to list our datasets. The datasets in turn contain files of interest. STREAM_ID=11586 curl -o datasets.json -X GET &quot;https://terraref.org/clowder/api/geostreams/datapoints?stream_id=${STREAM_ID}&quot; After the call succeeds, a file named datasets.json is created containing the returned JSON object. As part of the JSON object there are one or more properties fields containing source_dataset parameters. properties: { dataset_name: &quot;Thermal IR GeoTIFFs - 2016-05-09__12-07-57-990&quot;, source_dataset: &quot;https://terraref.org/clowder/datasets/59fc9e7d4f0c3383c73d2905&quot; }, The URL of each source_dataset can be used to view the dataset in Clowder. The datasets can also be filtered by date. The following filters out datasets that are outside of the range of January 2, 2017 through June 20, 2017. curl -o datasets.json -X GET &quot;https://terraref.org/clowder/api/geostreams/datapoints?stream_id=${STREAM_ID}&amp;since=2017-01-02&amp;until=2017-06-10&quot; 9.5.5 Getting file paths from dataset Now that we know what the dataset URLs are, we can use the URLs to query the API for file IDs in addition to their names and paths. Note the the URL has changed from our previous examples now that we’re using the URLs returned by the previous call. SOURCE_DATASET=&quot;https://terraref.org/clowder/datasets/59fc9e7d4f0c3383c73d2905&quot; curl -o files.json -X GET &quot;${SOURCE_DATASET}/files&quot; As before, we will have a file containing the returned JSON, named files.json in this case. The returned JSON consists of a list of the files in the dataset with their IDs, and other data if available: [ { size: &quot;346069&quot;, date-created: &quot;Fri Nov 03 11:51:13 CDT 2017&quot;, id: &quot;59fc9e814f0c3383c73d2962&quot;, filepath: &quot;/home/clowder/sites/ua-mac/Level_1/ir_geotiff/2016-05-09/2016-05-09__12-07-57-990/ir_geotiff_L1_ua-mac_2016-05-09__12-07-57-990.png&quot;, contentType: &quot;image/png&quot;, filename: &quot;ir_geotiff_L1_ua-mac_2016-05-09__12-07-57-990.png&quot; }, { size: &quot;1231298&quot;, date-created: &quot;Fri Nov 03 11:51:16 CDT 2017&quot;, id: &quot;59fc9e844f0c3383c73d2980&quot;, filepath: &quot;/home/clowder/sites/ua-mac/Level_1/ir_geotiff/2016-05-09/2016-05-09__12-07-57-990/ir_geotiff_L1_ua-mac_2016-05-09__12-07-57-990.tif&quot;, contentType: &quot;image/tiff&quot;, filename: &quot;ir_geotiff_L1_ua-mac_2016-05-09__12-07-57-990.tif&quot; } ] 9.5.6 Retrieving the files Given that a large number of files may be contained in a dataset, it may be desirable to automate the process of pulling down files to the local system. For each file to be retrieved, the unique file ID is needed on the URL. FILE_NAME=&quot;ir_geotiff_L1_ua-mac_2016-05-09__12-07-57-990.tif&quot; FILE_ID=59fc9e844f0c3383c73d2980 curl -o &quot;${FILE_NAME}&quot; -X GET &quot;https://terraref.org/clowder/api/files/${FILE_ID}&quot; This call will cause the server to return the contents of the file identified in the URL. This file is then stored locally in *ir_geotiff_L1_ua-mac_2016-05-09__12-07-57-990.tif*. "]]
