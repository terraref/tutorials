# Retrieve source RGB image files

## Learning Objective

In this chapter you will learn:

- Find and retrieve a list of available sensor names 
- Retrieve files by downloading them to your system

## Introduction

In this chapter we will show how to use the [Python](https://www.python.org/) `terrutils` library to retrieve the list of available sensor names.
The `terrautils` library provides a number of functions that can be used to perform different actions with data that is stored in [TerraRef](https://terraref.ncsa.illinois.edu/clowder/). 
The names of the sensors we retrieve using  `TerraUtils` can provide information on what types of Level 1 data is available.

Our examples also show how to retrieve files of interest from `TerraUtils` by using the available [API](https://clowder.ncsa.illinois.edu/swagger/?url=https://terraref.ncsa.illinois.edu/clowder/swagger) (Application Programming Interface).

## Getting Started

First, we will need to install the `terrautils` library into the Python environment.
We can do this by using the `pip` utility to install the library from pypi.
Simple run `pip install terrautils` in a terminal to install `terrautils`.
All the `terrautils` functions are now available in Python, although we will only use a very limited number of them.

## Retrieving sensor names

In this section we retrieve the names of different sensor types that are available. 
This will allow you to understand what files may be avaialable other than just those containing RBG image data.

We will first be using the `get_sensor_list` function to retrieve all the data on available sensors. 
We will then use the `unique_sensor_names` function to extract only the sensor names from the data we just retrieved.

```{python py_get_sensor_data, eval=FALSE}
from terrautils.products import get_sensor_list, unique_sensor_names

url = 'https://terraref.ncsa.illinois.edu/clowder/'
key = ''

sensors = get_sensor_list(None, url, key)
names = unique_sensor_names(sensors)
```

The variable `names` will now contain the list of all available sensors.

## Retrieving the images

Once we have a list of files and their IDs we can retrieve them one-by-one.
We do this by creating a URL that identifies the file to retrieve, making the API call to retrieve the file contents, and writing the contents to disk.

To create the correct URL we start with the one defined before and attach the keyword '/files/' followed by the ID of each file. 
For example, assuming we have a file ID of '111', the final URL for retrieving the file would be: 

``` {sh eval=FALSE}
https://terraref.ncsa.illinois.edu/clowder/api/files/111
```

By looping through each of our files, and using their ID and filename, we can retrieve the files from the server and store them locally. 

We are streaming the data returned from our server request (`stream=True` in the code below) due to the high probability of large file sizes.
If the `stream=True` parameter was omitted the file's entire contents would be in the `r` variable which could then be written to the local file.

To illustrate how this might work we are going to pre-populated an array of file names and their associated Clowder IDs.

```{python py_sample_images_data, eval=FALSE}
files = [ {"id": "5c507cb74f0c4b0cbe6705f2",
           "filename": "rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_right.tif"}, 
          {"id": "5c507cb84f0cfd2aedf5a75a",
           "filename": "rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_left.tif"},
          {"id": "5c507eaf4f0c4b0cbe6716cd",
           "filename": "rgb_geotiff_L1_ua-mac_2018-05-05__11-35-13-442_left.tif"},
          {"id": "5c507eaf4f0cfd2aedf5b680",
           "filename": "rgb_geotiff_L1_ua-mac_2018-05-05__11-37-40-442_right.tif"}
          ]
```

The following code shows how to download the image files. 
First we format the base URL for our query allowing us to reuse it for each file.
Next we loop through our array and create a customized URL while making the call to fetch the data using the `requests` interface.
Finally we open the output file and use a loop to write the retrieved data.

```{python py_fetch_image_files, eval=FALSE}
import requests
from io import open

# We are using the same `url` and `key` variables declared in the previous example above.
filesurl = url + '/files/'
params={ 'key': key }

for f in files:
  r = requests.get(filesurl + f["id"], params=params, stream=True)
  with open(f["filename"], 'wb') as o:
        for chunk in r.iter_content(chunk_size=1024): 
            if chunk:
                o.write(chunk)
     
```

The images are now stored on the local file system.

## Sample Images

Below are examples of images captured approximately one month apart [^1] [^2]

| Date        | Images                                              |
|-------------|-----------------------------------------------------|
| May 4, 2018 | ![](https://user-images.githubusercontent.com/45463434/52152314-38d23a80-2633-11e9-99c8-983d79d7bcda.png)  ![](https://user-images.githubusercontent.com/45463434/52152386-78992200-2633-11e9-9cca-ab6e7d09614d.png) |
| Jun 2, 2018 | ![](https://user-images.githubusercontent.com/45463434/52152360-65865200-2633-11e9-9397-14429a192868.png)  ![](https://user-images.githubusercontent.com/45463434/52152339-530c1880-2633-11e9-8b64-50ffdf8215be.png) |

[^1]: May 4, 2018 - rgb_geotiff_L1_ua-mac_2018-05-04__13-07-04-077_right.tif,rgb_geotiff_L1_ua-mac_2018-05-04__13-07-04-077_left.tif
[^2]: Jun 2, 2018 - rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_right.tif,rgb_geotiff_L1_ua-mac_2018-06-02__14-12-05-077_left.tif


<!-- The following is the 'correct' way of retrieving file names and IDS based upon a site and sensor name 



## Objective: To be able to demonstrate how to locate and retrieve RGB image files

This vignette shows how to locate and retrieve image files from your site for a specific date range using Python.
We will be searching for and retrieving [ublicly available images associated with growing Season 6 from the University of Arizona's [Maricopa Agricultural Center](http://cals-mac.arizona.edu/). 
Note that the search in this vignette will not return any results, but we wull walk you through the process so that you can query your own data.
The files are stored online on the data management system Clowder, which is accessed using an API. 

After completing this vignette it should be possible to search for and retrieve other files through the use of the API.

As an added bonus we've also included an exmple of how to retrieve the list of available sensor names through the API.
By using the sensor names returned, it's possible to retrieve other files containing the data the sensors have collected.

**Requirements** 

* Python 
* the terrautils library
    + this can be installed from pypi by running `pip install terrautils` in the terminal
* an API key to access these data

The API key is a string that gets generated upon request through your Clowder account. 
Existing API keys will work with this vignette.
The following steps can help you get a new API key

1. first register with Clowder at "https://terraref.ncsa.illinois.edu/clowder/" site
      a) click the `Login` button and wait for the login screen to appear
      b) then select the `Sign up` button and enter an email address you have access to
2. an email is sent to the entered address with instructions for completing the registration process
3. once registration is complete
      c) log  into Clowder and select the `View profile` menu option from the drop-down that is near the search control
      d) enter a name and click the `+ Add` button under "User API Keys" heading in the profile page to generate a new key
4. copy the key shown for use in this vignette

## Locating the images

To begin looking for files, a sensor name and site name are needed. 
We will be using 'RGB GeoTIFFs Datasets' as the sensor name and 'MAC Field Scanner Season 6 Range 20 Column 6' as the site name. 
Later in this vignette we show how to retrieve the list of available sensors.
When seraching for your images, you will want to use your site name.

The URL string points to the API to query. 
In this case we'll be using "https://terraref.ncsa.illinois.edu/clowder/api".
We will be using an empty key for this example since we are querying public data.
You will want to use the key you created for your Clowder account when querying your data.

As mentioned in the Objectives, we are limiting the search timeframe through the use of the `since` and `until` parameters.
These parameters can be omitted to search for all available images (or all other files by changing the sensor).

```{python py_get_file_listing, eval=FALSE}
from terrautils.products import get_file_listing

url = 'https://terraref.ncsa.illinois.edu/clowder/'
key = ''
sensor = 'RGB GeoTIFFs Datasets'
sitename = 'MAC Field Scanner Season 6 Range 20 Column 6'
files = get_file_listing(None, url, key, sensor, sitename,
                   since='2018-05-01', until='2018-05-31')
```

The `files` variable now contains an array of all the file in the datasets that match the sensor in the plot for the month of May. 
As with this example, when performing you own queries it's possible that there are no matches found and the `files` array would be empty.

-->