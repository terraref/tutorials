[
["vignettes-introduction.html", "Get Weather Data 1 Vignettes Introduction", " Get Weather Data 1 Vignettes Introduction "],
["accessing-trait-data-in-r.html", "2 Accessing trait data in R", " 2 Accessing trait data in R "],
["introduction.html", "3 Introduction", " 3 Introduction The objective of this vignette is to demonstrate to users how to query TERRA REF trait data using the traits package. The traits package allows users to easily pass query parameters into a R function, and returns the data in a tabular format that can be analyzed. Through this vignette, users will learn how to query and visualize season 6 canopy height data for May 2018. In addition, users will also be shown how to find more information on a season, such as available traits and dates, when performing their own queries. "],
["getting-started.html", "4 Getting Started", " 4 Getting Started First, you will need to install and load the traits package from github. devtools::install_github(&#39;terraref/traits&#39;, force = TRUE) library(traits) "],
["how-to-query-trait-data.html", "5 How to query trait data 5.1 Setting options 5.2 An example: Season 6 canopy height data 5.3 Time series of canopy height", " 5 How to query trait data 5.1 Setting options The function that you will be using to perform your queries is betydb_query. Options can be set to reduce the number of arguments that need to be passed into the function. Note: the betydb_key option only needs to be set when accessing non-public data. We will be using public data, so this option does not need to be set. However, when needed, pass in the API key that you were assigned when you first registered for access to the TERRA REF database. The key should be kept private and saved to a file named .betykey in your current directory. If you are having trouble locating your API key, you can go to https://terraref.ncsa.illinois.edu/bety/users. options(# betydb_key = &#39;Your API Key&#39;, # to access non-public data betydb_url = &quot;https://terraref.ncsa.illinois.edu/bety/&quot;, betydb_api_version = &#39;v1&#39;) 5.2 An example: Season 6 canopy height data The following is an example of how to query season 6, canopy height data for May 2018. canopy_height &lt;- betydb_query(table = &quot;search&quot;, trait = &quot;canopy_height&quot;, sitename = &quot;~Season 6&quot;, date = &quot;~2018 May&quot;, limit = &quot;none&quot;) A breakdown of the above query: table = &quot;search&quot; Specify a table to query with the table parameter. Trait data may be queried using the search table. trait = &quot;canopy_height&quot; Specify the trait of interest with the trait parameter. Trait names must be expressed exactly as they are in the TERRA REF databse. So passing in Canopy height instead of canopy_height would give NULL results. More information on how to determine available traits for a season can be found below under How to query other seasons, traits, and dates. sitename = &quot;~Season 6&quot; Indicate the sites that you would like to query using the sitename parameter. A tilde ~ is used in this query to get all sitenames that contain Season 6 date = &quot;~2018 May&quot; Indicate the date of data collection using the date parameter. A tilde ~ is used in this query to get all records that have a collection date that contains 2018 May limit = &quot;none&quot; Indicate the maximum numnber of records you would like returned with the limit parameter. We want all records for this query, so we set limit to none. 5.3 Time series of canopy height Here is an example of how to visualize the data that we just queried. #load in necessary packages library(ggplot2) library(lubridate) #plot a time series of canopy height ggplot(data = canopy_height, aes(x = lubridate::yday(lubridate::ymd_hms(raw_date)), y = mean)) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + xlab(&quot;Day of Year&quot;) + ylab(&quot;Plant Height&quot;) + guides(color = guide_legend(title = &#39;Genotype&#39;)) + theme_bw() "],
["may-2018-season-6-summary.html", "6 May 2018 Season 6 Summary", " 6 May 2018 Season 6 Summary The TERRA REF database contains other trait data for May 2018 of season 6. Each trait was measured using a specific method. Here is a summary of available traits and their corresponding methods of measurement. trait method_name number_of_observations 1 canopy_cover Canopy Cover Estimation from Field Scanner RGB images 1170 2 canopy_height Scanner 3d ply data to height 133 "],
["how-to-query-other-seasons-traits-and-dates.html", "7 How to query other seasons, traits, and dates", " 7 How to query other seasons, traits, and dates You can query other seasons, traits, and dates by changing the season number, trait name, and date in the example query. If you are unsure of what traits or dates are available for a season, you can use the following R code to get a subset of a season and figure out what specific dates and traits are available. To broaden your queries, remove specific parameters. For example, in order to get all of season 2’s data for October 2016, remove the trait parameter. #get all of season 2 data for October 2016 season_2_sub &lt;- betydb_query(table = &quot;search&quot;, sitename = &quot;~Season 2&quot;, date = &quot;~2016 Oct&quot;, limit = &quot;none&quot;) #get traits available for the subset of season 2 data traits &lt;- unique(season_2_sub$trait) print(traits) [1] &quot;NDVI&quot; #filter for NDVI trait records ndvi &lt;- dplyr::filter(season_2_sub, trait == &#39;NDVI&#39;) #get unique dates for NDVI records ndvi_dates &lt;- unique(ndvi$date) print(ndvi_dates) [1] &quot;2016 Oct 1 (America/Phoenix)&quot; &quot;2016 Oct 25 (America/Phoenix)&quot; "],
["objective-to-be-able-to-demonstrate-how-to-get-terra-ref-meteorological-data.html", "8 Objective: To be able to demonstrate how to get TERRA REF meteorological data", " 8 Objective: To be able to demonstrate how to get TERRA REF meteorological data This vignette shows how to read weather data for the month of January 2017 from the weather station at the University of Arizona’s Maricopa Agricultural Center into R. These data are stored online on the data management system Clowder, which is accessed using an API. Data across time for two of the weather variables, temperature and precipitation, are plotted in R. Lastly, how to get the list of all possible weather variables is demonstrated. 8.0.1 Using the API to get data In order to access the data, we need to contruct a URL that links to where the data is located on Clowder. The data is then pulled down using the API, which “receives requests and sends responses” , for Clowder. 8.0.2 The structure of the database The meteorological data that is collected for the TERRA REF project is contained in multiple related tables, also know as a relational database. The first table contains data about the sensor that is collecting data. This is then linked to a stream table, which contains information about a datastream from the sensor. Sensors can have multiple datastreams. The actual weather data is in the third table, the datapoint table. A visual representation of this structure is shown below. In this vignette, we will be using data from a weather station at the Maricopa Agricultural Center, with datapoints for the month of January 2017 from a certain sensor. These data are five minute summaries aggregated from observations taken every second. 8.0.3 Creating the URLs for all data table types All URLs have the same beginning (https://terraref.ncsa.illinois.edu/clowder/api/geostreams), then additional information is added for each type of data table as shown below. Station: /sensors/sensor_name=[name] Sensor: /sensors/[sensor number]/streams Datapoints: /datapoints?stream_id=[datapoints number]&amp;since=[start date]&amp;until=[end date] A certain time period can be specified for the datapoints. For example, below are the URLs for the particular data being used in this vignette. These can be pasted into a browser to see how the data is stored as text using JSON. Station: https://terraref.ncsa.illinois.edu/clowder/api/geostreams/sensors?sensor_name=UA-MAC+AZMET+Weather+Station Sensor: https://terraref.ncsa.illinois.edu/clowder/api/geostreams/sensors/438/streams Datapoints: https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2017-01-02&amp;until=2017-01-31 Possible sensor numbers for a station are found on the page for that station under “id:”, and then datapoints numbers are found on the sensor page under “stream_id:”. 8.0.4 Download data using the command line Data can be downloaded from Clowder using the command line program Curl. If the following is typed into the commmand line, it will download the datapoints data that we’re interested in as a file which we have chosen to call spectra.json. curl -o spectra.json -X GET https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2017-01-02&amp;until=2017-01-31 8.0.5 Read in data using R The same data can be accessed with the URL using the R package jsonlite. We are calling that library along with several others that will be used to clean and plot the data. The data is read in by the fromJSON function as a dataframe that also has two nested dataframes, called properties and geometries. library(dplyr) library(ggplot2) library(jsonlite) library(lubridate) weather_all &lt;- fromJSON(&#39;https://terraref.ncsa.illinois.edu/clowder/api/geostreams/datapoints?stream_id=46431&amp;since=2017-01-02&amp;until=2017-01-31&#39;, flatten = FALSE) The geometries dataframe is then pulled out from these data, which contains the datapoints from this stream. This is combined with a transformed version of the end of the time period from the stream. weather_data &lt;- weather_all$properties %&gt;% mutate(time = ymd_hms(weather_all$end_time)) The temperature data, which is five minute averages for the entire month of January 2017, is used to calculate the growing degree days for each day. Growing degree days is a measurement that is used to predict when certain plant developmental phases happen. This new dataframe will be used in the last vignette to synthesize the trait, weather, and image data. daily_values = weather_data %&gt;% mutate(date = as.Date(time), air_temp_converted = air_temperature - 273.15) %&gt;% group_by(date) %&gt;% summarise(min_temp = min(air_temp_converted), max_temp = max(air_temp_converted), gdd = ifelse(sum(min_temp, max_temp) / 2 &gt; 10, (max_temp + min_temp) / 2 - 10, 0)) 8.0.6 Plot data using R The five minute summary weather variables in the weather_data dataframe can be plotted across time, as shown below for temperature and precipitation. theme_set(ggthemes::theme_few()) ggplot(data = weather_data) + geom_point(aes(x = time, y = air_temperature), size = 0.1) + labs(x = &quot;Date&quot;, y = &quot;Temperature (K)&quot;) ggplot(data = weather_data) + geom_point(aes(x = time, y = precipitation_rate), size = 0.1) + labs(x = &quot;Date&quot;, y = &quot;Precipitation (kg/m2s)&quot;) 8.0.7 Get all available weather variables The weather variables that are available from these datapoints data are extracted below from the column names of the dataframe that we read in earlier. Any of these variables that are of interest can be analyzed and plotted. cols = colnames(weather_data) cols[!cols %in% c(&quot;source&quot;, &quot;source_file&quot;, &quot;time&quot;)] ## [1] &quot;wind_speed&quot; ## [2] &quot;eastward_wind&quot; ## [3] &quot;northward_wind&quot; ## [4] &quot;air_temperature&quot; ## [5] &quot;relative_humidity&quot; ## [6] &quot;precipitation_rate&quot; ## [7] &quot;surface_downwelling_shortwave_flux_in_air&quot; ## [8] &quot;surface_downwelling_photosynthetic_photon_flux_in_air&quot; You should now be able to find, get, and use weather data from the TERRA REF project via Clowder. "],
["objective-to-be-able-to-demonstrate-how-to-locate-and-retrieve-rgb-image-files.html", "9 Objective: To be able to demonstrate how to locate and retrieve RGB image files 9.1 Locating the images", " 9 Objective: To be able to demonstrate how to locate and retrieve RGB image files This vignette shows how to locate and retrieve image files associated with growing Season 6 from the University of Arizona’s Maricopa Agricultural Center using Python. The files are stored online on the data management system Clowder, which is accessed using an API. We will be working with the image files generated during the month of May by limiting the requests to that time period. After completing this vignette it should be possible to search for and retrieve other files through the use of the API. As an added bonus we’ve also included an exmple of how to retrieve the list of available sensor names through the API. By using the sensor names returned, it’s possible to retrieve other files containing the data the sensors have collected. requirements * Python 3 * the terrautils library * this can be installed from pypi by running pip install terrautils in the terminal * an API key to access these data The API key is a string that gets generated upon request through your Clowder account. Existing API keys will work with this vignette. To get a new API key it is necessary to first register with Clowder at “https://terraref.ncsa.illinois.edu/clowder/”. First click the Login button and wait for the login screen to appear. Then select the Sign up button and enter an email address you have access to. An email is sent to the entered address with instructions for completing the registration process. Once registration is complete, log into Clowder and select the View profile menu option from the drop-down that is near the search control. By clicking the + Add button under “User API Keys” heading in the profile page, a new key is gnerated. 9.1 Locating the images To begin looking for files, a sensor name and site name are needed. We will be using ‘RGB GeoTIFFs Datasets’ as the sensor name and ’’ as the site name. Later in this vignette we show how to retrieve the list of available sensors. As mentioned in the overview, the url string will point to the API to use. In this case we’ll be using “https://terraref.ncsa.illinois.edu/clowder/api” and the key will be the one you created for your Clowder account. from terrautils.products import get_file_listing url = &#39;https://terraref.ncsa.illinois.edu/clowder/api&#39; key = &#39;YOUR_KEY_GOES_HERE&#39; sensor = &#39;RGB GeoTIFFs Datasets&#39; sitename = &#39;&#39; files = get_file_listing(None, url, key, sensor, sitename, since=&#39;2018-05-01&#39;, until=&#39;2018-05-31&#39;) The files variable now contains an array of all the file in the datasets that match the sensor in the plot for the month of May. When performing you own queries it’s possible that there are no matches found and the files array would be empty. "],
["retrieving-the-images.html", "10 Retrieving the images", " 10 Retrieving the images Now that we have a list of files we can retrieve them one-by-one. We do this by creating a URL that identifies the file to retrieve, making the API call to retrieve the file contents, and writing the contents to disk. To create the correct URL we start with the one defined before and attach the keyword ‘/files/’ followed by the ID of each file. Assuming we have a file ID of ‘111’, the final URL for retrieving the file would be: https://terraref.ncsa.illinois.edu/clowder/api/files/111 By looping through each of the returned files from the previous example, and using their ID and filename, we can retrieve the files from the server and store them locally. We are streaming the data returned from our server request (stream=True in the code below) due to the high probability of large file sizes. If the stream=True parameter was omitted the file’s entire contents would be in the r variable which could then be written to the local file. # We are using the same `url` and `key` variables declared in the previous example above. filesurl = url + &#39;/files/&#39; params={ &#39;key&#39;: key } for f in files: r = requests.get(fileurl + f.id, params=params, stream=True) with open(f.filename, &#39;wb&#39;) as o: for chunk in r.iter_content(chunk_size=1024): if chunk: o.write(chunk) The images are now stored on the local file system. "],
["retrieving-sensor-names.html", "11 Retrieving sensor names", " 11 Retrieving sensor names In this section we retrieve the names of different sensor types that are available. This will allow you to retrieve files other than those containing RBG image data. # We are using the same `url` and `key` variables declared in the previous example above. from terrautils.products import get_sensor_list, unique_sensor_names sensors = get_sensor_list(None, url, key) names = unique_sensor_names(sensors) The variable names will now contain the list of all available sensors. Using these sensor names it’s possible to use the above search to locate and then retrieve additional data files. Substitute the new sensor name for ‘RGB GeoTIFFs Datasets’ where the variable sensor is assigned above. "],
["synthesis-vignette.html", "12 Synthesis Vignette", " 12 Synthesis Vignette "]
]
